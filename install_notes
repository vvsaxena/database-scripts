Installing kafka on 172.28.96.241

yum install java-1.8.0-openjdk.x86_64 -y
java -version
vi /etc/profile
add following
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
export JRE_HOME=/usr/lib/jvm/jre
source /etc/profile
wget http://www-us.apache.org/dist/kafka/2.0.0/kafka_2.11-2.0.0.tgz
tar xvf kafka_2.11-2.0.0.tgz
mv kafka_2.11-2.0.0 /opt
cd /opt/kafka_2.11-2.0.0
bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
netstat -tlpn
bin/kafka-server-start.sh config/server.properties &
Adding a topic test
bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test
List the topics
bin/kafka-topics.sh --list --zookeeper localhost:2181
Watch the incomming messages in topic
bin/kafka-console-consumer.sh --bootstrap-server 172.28.96.241:9092 --topic 172.28.97.159.vishal.test --from-beginning
===================================================================================================
Installing maxwell first
wget https://github.com/zendesk/maxwell/releases/download/v1.18.0/maxwell-1.18.0.tar.gz
gunzip maxwell-1.18.0.tar.gz
tar xvf gunzip maxwell-1.18.0.tar
cd maxwell-1.18.0/bin
For viewing the mysql changes on standard output , assuming mysql is running on 172.28.97.159
./maxwell --user='maxwell' --password='maxwell' --host='172.28.97.159' --producer=stdout
Use javascript file to filter the regular insert/updates/deletes 
 ./maxwell --user='maxwell' --password='maxwell123' --host=172.28.97.151 --producer=stdout --javascript testfile
and here is the testfile
------------------------------------------------
function process_row(row) {
                if ( row.type == "insert" ) {
                                row.suppress();
                        }
                else if ( row.type == "update" ) {
                        row.suppress();
                } else if ( row.type == "delete" ) {
                                row.suppress();
                        }
}
---------------------------------------------------------


Make sure server id is set in my.cnf
to send the messages to kafka
./maxwell --user='maxwell' --password='maxwell' --host='172.28.97.159' --producer=kafka --kafka.bootstrap.servers=172.28.96.241:9092 --kafka_topic=test
===================================================================================================
Now installing debezium on another machine vishalmysql2
docker pull debezium/connect
run debezium with kafka setup ( pay attention to --network host)
docker run -it --network host --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my-connect-configs -e OFFSET_STORAGE_TOPIC=my-connect-offsets -e ADVERTISED_HOST_NAME=vishalmysql2.dev.adcinternal.com -e BOOTSTRAP_SERVERS=172.28.96.241:9092 debezium/connect
check if its running or not
docker ps -a
Now run the connector manually ( assuming that vishal is a database inside mysql server)
curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" 172.17.0.1:8083/connectors/ -d '{ "name": "inventory-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "172.28.97.159", "database.port": "3306", "database.user": "maxwell", "database.password": "maxwell", "database.server.id": "123", "database.server.name": "172.28.97.159", "database.whitelist": "vishal", "database.history.kafka.bootstrap.servers": "172.28.96.241:9092", "database.history.kafka.topic": "dbhistory.vishal" } }'

If this fails with timezone issues as mentioned below

{"error_code":400,"message":"Connector configuration is invalid and contains the following 1 error(s):\nUnable to connect: The server time zone value 'EDT' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support.\nYou can also find the above list of errors at the endpoint `/{connectorType}/config/validate`"}

Ran following SQL in mysql to resolve this issue 
SET GLOBAL time_zone = '+3:00';

[root@vishalmysql2 vsaxena]# curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" 172.17.0.1:8083/connectors/ -d '{ "name": "inventory-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "172.28.97.159", "database.port": "3306", "database.user": "maxwell", "database.password": "maxwell", "database.server.id": "123", "database.server.name": "172.28.97.159", "database.whitelist": "vishal", "database.history.kafka.bootstrap.servers": "172.28.96.241:9092", "database.history.kafka.topic": "dbhistory.vishal" } }'
HTTP/1.1 201 Created
Date: Thu, 20 Sep 2018 16:06:01 GMT
Location: http://172.17.0.1:8083/connectors/inventory-connector
Content-Type: application/json
Content-Length: 497
Server: Jetty(9.2.24.v20180105)

{"name":"inventory-connector","config":{"connector.class":"io.debezium.connector.mysql.MySqlConnector","tasks.max":"1","database.hostname":"172.28.97.159","database.port":"3306","database.user":"maxwell","database.password":"maxwell","database.server.id":"123","database.server.name":"172.28.97.159","database.whitelist":"vishal","database.history.kafka.bootstrap.servers":"172.28.96.241:9092","database.history.kafka.topic":"dbhistory.vishal","name":"inventory-connector"},"tasks":[],"type":null}[root@vishalmysql2 vsaxena]#

Now make some schema changes/inserts on mysql and on Kafka host see the changes in watcher

For all changes specific to a table test in database vishal
bin/kafka-console-consumer.sh --bootstrap-server 172.28.96.241:9092 --topic 172.28.97.159.vishal.test --from-beginning

For all changes specific to Whole server 172.28.97.159 , show all schema changes and grants
bin/kafka-console-consumer.sh --bootstrap-server 172.28.96.241:9092 --topic 172.28.97.159 --from-beginning

For all only schema changes
bin/kafka-console-consumer.sh --bootstrap-server 172.28.96.241:9092 --topic dbhistory.vishal --from-beginning
============================================================================================================
Installing elasticsearch/logstash and transferring mysql data to elasticsearch

1.Install elasticsearch 
    yum install elasticsearch
    systemctl start elasticsearch
    http://localhost:9200

2.Download the logstash tar file ( don't install with yum )

   wget https://artifacts.elastic.co/downloads/logstash/logstash-6.6.0.tar.gz
   gunzip and untar it
   create a simlink logstash to logstash-6.6.0

3. download the mysql-connector for jdbc
   mysql-connector-java-5.1.6.tar.gz  
   unzip and untar it and set the correct path of jar file in logstash.conf ( next step)

4.Prepare the logstash.conf file like this

input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/vishal1"
    # The user we wish to execute our statement as
    jdbc_user => "testuser"
    jdbc_password => "test123@=yhgf"
    # The path to our downloaded jdbc driver
    jdbc_driver_library => "/home/local/CORP/vsaxena/mysql-connector-java-5.1.6/mysql-connector-java-5.1.6-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    # our query
    statement => "SELECT * FROM adhoc_sql_requests"
    }
  }
output {
  stdout { codec => json_lines }
  elasticsearch {
  "hosts" => "localhost:9200"
  "index" => "test-migrate"
  "document_type" => "data"
  }
}

5. Finally run the pipe line for data transfer from mysql to elasticsearch

   logstash/bin/logstash -f logstash.conf

6. Verify with 

   curl -XPOST 'http://localhost:9200/test-migrate/_search?pretty=true' -d '{}'
===============================================================================================================================
logstash.template file used by another script to generate logstash transfer from mysql to elasticsearch

input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/REPL_DB"
    # The user we wish to execute our statement as
    jdbc_user => "testuser"
    jdbc_password => "test123@=yhgf"
    # The path to our downloaded jdbc driver
    jdbc_driver_library => "/home/local/CORP/vsaxena/mysql-connector-java-5.1.6/mysql-connector-java-5.1.6-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    # our query
    statement => "SELECT REPL_QRY from REPL_TABLE"
    }
  }
output {
  stdout { codec => json_lines }
  elasticsearch {
  "hosts" => "localhost:9200"
  "index" => "altx_alllocal"
  "document_type" => "REPL_TYPE"
  "document_id" => "%{docid}"
  }
}
=====================================================================================================================================
Script which used logstash.template file to start the transfer process , it uses primary keys to add as document_id in elasticsearch

send_data.sh

#!/bin/sh
####
## curl -XDELETE http://localhost:9200/altx_alllocal/invoice/AWjtltSYPfmU_9k6VmnL/?pretty
## curl -XDELETE http://localhost:9200/<Index>/<Type>/<Document-id>/?pretty
###
dbList=$(mysql --login-path=root --skip-column-names -e"show databases like 'altx_%'")
echo "$dbList"|while read DB
do
        tableList=$(mysql --login-path=root --skip-column-names -e"show tables" --database $DB)
        echo "$tableList"|while read TBL
        do
                SQL="SELECT GROUP_CONCAT(COLUMN_NAME)
FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
WHERE
TABLE_SCHEMA = \"$DB\"
AND CONSTRAINT_NAME='PRIMARY' and TABLE_NAME=\"$TBL\""
                pkcols=$(echo "$SQL"|mysql --login-path=root  --skip-column-names --database $DB)
                colList=$(mysql --login-path=root --skip-column-names -e"select group_concat(column_name) from information_schema.columns where table_name=\"$TBL\" and table_schema=\"$DB\"")
                ESTYPE="${DB}_${TBL}"
                sed "s/REPL_DB/$DB/;s/REPL_TABLE/$TBL/;s/REPL_TYPE/$ESTYPE/;s/REPL_QRY/\'$DB\' as \'database\',concat($pkcols) as docid,$colList/" logstash.template > logstash.conf
                logstash/bin/logstash -f logstash.conf
        done

done
=========================================================================================================================================
To sync the ongoing changes for inserts/updates/deletes , start maxwell daemon to read binlog changes in mysql_changes file like this

./maxwell --user='testuser'  --password='test123@=yhgf' --host=localhost --producer=file --output_file=mysql_changes --output_binlog_position

and then use this script "update_data.sh" to sync changes to elasticsearch
./update_data.sh mysql_changes

Script is here

#!/bin/sh
###Functions starts here
process_insert_thru_curl() {
localItem=$4
localDB=$1
localtype=$2
localtbl=$3
SQL="SELECT GROUP_CONCAT(COLUMN_NAME)
FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
WHERE
TABLE_SCHEMA = \"$localDB\"
AND CONSTRAINT_NAME='PRIMARY' and TABLE_NAME=\"$localtbl\""
pkcols=$(echo "$SQL"|mysql --login-path=root  --skip-column-names --database $DB)
ESTYPE="${localDB}_${localtbl}"
PAT=""
for colname in `echo "$pkcols"|tr ',' ' '`
do
        cmnd=$(echo "echo \"\$localItem\"|jq -r '.data|.$colname'")
        dataItem=$(eval "$cmnd"|sed "s/\"/'/g")
        PAT=$PAT`echo -e "$dataItem"`
done
DATA=$(echo "$localItem"|jq '.data'|egrep -v "^{|^}")
DATA=$DATA`echo -e "\n  ,\"database\": \"$localDB\","`
DATA=$DATA`echo -e "\n  \"docid\": \"$PAT\""`

ESCOMMAND="curl -X PUT \"http://localhost:9200/altx_alllocal/$ESTYPE/$PAT?refresh=true&pretty\" -H 'Content-Type: application/json' -d'
{
$DATA
}' "

commOutput=$(eval "$ESCOMMAND")
if echo "$commOutput"|egrep "\"created\" : false|\"created\" : true" > /dev/null
then
        :
else
        echo "Failed during UPDATE or INSERT..."
        global_comm_status=1
        touch /home/local/CORP/vsaxena/DO_NOT_PROCEED
        cmnd=$(echo "echo \"\$localItem\"|jq -r '.position'")
        lastPos=$(eval "$cmnd")
        echo "Exiting at $lastPos"
        exit 1
fi

}

process_insert() {
localItem=$4
localDB=$1
localtype=$2
localtbl=$3
SQL="SELECT GROUP_CONCAT(COLUMN_NAME)
FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
WHERE
TABLE_SCHEMA = \"$localDB\"
AND CONSTRAINT_NAME='PRIMARY' and TABLE_NAME=\"$localtbl\""
pkcols=$(echo "$SQL"|mysql --login-path=root  --skip-column-names --database $DB)
ESTYPE="${localDB}_${localtbl}"
pkcount=$(echo "$pkcols"|tr ',' '\n'|wc -l|awk '{print $1}')
colList=$(mysql --login-path=root --skip-column-names -e"select group_concat(column_name) from information_schema.columns where table_name=\"$localtbl\" and table_schema=\"$localDB\"")
WHERE_COND=""
start=1
for colname in `echo "$pkcols"|tr ',' ' '`
do
        cmnd=$(echo "echo \"\$localItem\"|jq '.data|.$colname'")
        dataItem=$(eval "$cmnd"|sed "s/\"/'/g")
        if [ $start -eq 1 ]
        then
                WHERE_COND=$WHERE_COND`echo -e " WHERE $colname = $dataItem"`
        else
                WHERE_COND=$WHERE_COND`echo -e " AND $colname = $dataItem"`
        fi
        start=`expr $start + 1`
done

sed "s/REPL_DB/$localDB/;s/REPL_TABLE/$localtbl/;s/REPL_TYPE/$ESTYPE/;s/REPL_WHERE/$WHERE_COND/;s/REPL_QRY/\'$localDB\' as \'database\',concat($pkcols) as docid,$colList/" logstash.template.update > logstash.conf

$LOGSTASH -f logstash.conf

}

process_delete() {
localItem=$4
localDB=$1
localtype=$2
localtbl=$3
ESTYPE="${localDB}_${localtbl}"
SQL="SELECT GROUP_CONCAT(COLUMN_NAME)
FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
WHERE
TABLE_SCHEMA = \"$localDB\"
AND CONSTRAINT_NAME='PRIMARY' and TABLE_NAME=\"$localtbl\""
pkcols=$(echo "$SQL"|mysql --login-path=root  --skip-column-names --database $DB)
PAT=""
for colname in `echo "$pkcols"|tr ',' ' '`
do
        cmnd=$(echo "echo \"\$localItem\"|jq '.data|.$colname'")
        dataItem=$(eval "$cmnd"|sed "s/\"/'/g")
        PAT=$PAT`echo -e "$dataItem"`
done
commOutput=$(eval "curl -XDELETE http://localhost:9200/altx_alllocal/$ESTYPE/$PAT?refresh=true")
echo $?
echo "$commOutput"
if echo "$commOutput"|grep "{\"found\":true" > /dev/null
then
        :
else
        echo "Failed during DELETE..."
        global_comm_status=1
        touch /home/local/CORP/vsaxena/DO_NOT_PROCEED
        cmnd=$(echo "echo \"\$localItem\"|jq -r '.position'")
        lastPos=$(eval "$cmnd")
        echo "Exiting at $lastPos"
        exit 1
fi
}

####Main Starts here
changeFile=$1
###extract for which database , table and type of event occured
global_comm_status=0
while read line
do
        DB=$(echo "$line"|jq -r '.database')
        evntype=$(echo "$line"|jq -r '.type')
        tbl=$(echo "$line"|jq -r '.table')

        if [ $evntype = "insert" ]
        then
                echo "Processing insert"
                ### Use process_insert function if you want logstash to do the sync
                process_insert_thru_curl "$DB" "$evntype" "$tbl" "$line"
        fi

        if [ $evntype = "update" ]
        then
                echo "Processing update"
                ### Use process_insert function if you want logstash to do the sync
                process_insert_thru_curl "$DB" "$evntype" "$tbl" "$line"
        fi

        if [ $evntype = "delete" ]
        then
                echo "Processing delete"
                process_delete "$DB" "$evntype" "$tbl" "$line"

        fi
done <<< "$(cat $changeFile)"
########################################################################
echo "Sync Finished with status : $global_comm_status"
if [ $global_comm_status -eq 0 ]
then
        mv $changeFile ${changeFile}.done
fi
########################################################################

===============================================================================================================================
This script run_sync_to_elasticsearch.sh calls the above script 

#!/bin/sh
#### Proceed only if previous sync was successfull
if [ -f /home/local/CORP/vsaxena/DO_NOT_PROCEED ]
then
        echo "Can't Proceed as previous sync did not succeed .."
        exit 1
fi
##################################################################
startPos=$1
scriptdir=/home/local/CORP/vsaxena
PAT=$(date +"%m%d%y%H%M%S")
transactionFile=$scriptdir/${PAT}_mysql_changes
MAXWELL=$scriptdir/maxwell-1.18.0/bin/maxwell
echo "Collecting changes from maxwell ......"
if [ ${#startPos} -eq 0 ]
then
        timeout 60 $MAXWELL --user='testuser'  --password='test123@=yhgf' --host=localhost --producer=file --output_file=$transactionFile --output_binlog_position
else
        echo "Staring with specified position ...$startPos"
        timeout 60 $MAXWELL --user='testuser'  --password='test123@=yhgf' --host=localhost --producer=file --init_position="$startPos" --output_file=$transactionFile --output_binlog_position
fi

sleep 5
echo "Now syncing elasticsearch.....with new changes"
if [ -s $transactionFile ]
then
        outstr=$($scriptdir/update_data.sh $transactionFile 2>&1)
        echo "$outstr"
fi
######################################################################################

========================================================================================================================================
